
<!-- saved from url=(0024)http://xingwang4nlp.com/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Xing Wang's Homepage</title>
<style type="text/css">
body{font-family: cambria, calibri, garamond, century, gulim, dotum, arial;font-size:16px;}
</style>
<style>
table {
font-family: cambria, calibri, garamond, century, gulim, dotum, arial;
border-collapse: collapse;
width: 100%;
font-size:17px;
}

</style>



</head><body>

<div align="center">
<table border="0" width="84%" cellspacing="0" cellpadding="0" id="table1">
<tbody><tr>
<td>

<div align="center">
<table border="0" width="99%" cellspacing="0" cellpadding="0" id="table2">
<tbody><tr>
<td>
<h1>Xing Wang (王 星)</h1>
<b>E-mail: </b>xingwsuda@gmail.com <br>

</td></tr><tr>
<td align="left" valign="top"><hr color="#000000" size="1">

<h2>Biography</h2>
<ul>
<li type="circle"> Xing Wang is a senior researcher at Tencent AI Lab. His research interests include  large language models, multimodal  sign language translation, neural/statistical machine translation, and biomedical NLP.  He has published over 40 papers in conferences and journals, including&nbsp;ACL, NeurIPS, ICLR, EMNLP, NAACL, AAAI, etc.&nbsp; He served as Area Chair for ACL, EMNLP, NAACL and ICLR. </li> 
</ul></td></tr>

<tr>
<td align="left" valign="top"><hr color="#000000" size="1">
<h2>Selected Publication</h2>(<a href="http://xingwang4nlp.com/publications.html">full list</a>, <a href="https://scholar.google.com/citations?user=6AqRKa0AAAAJ&amp;hl=en">Google Scholar</a>, * denotes corresponding author )

<h3>Large Language Model</h3>

<ul>
<li type="circle">Zicheng Lin, Tian Liang, Jiahao Xu, <b>Xing Wang</b>, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu Yang, Zhaopeng Tu. <font color="#0080FF">Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability</font>. <i>arixv 2025</i>. (<a href="https://arxiv.org/abs/2411.19943">pdf</a>)[<a href="https://github.com/chenzhiling9954/Critical-Tokens-Matter">code</a>]
</li>
</ul>

<ul>
<li type="circle">Zhiwei He, Zhaopeng Tu, <b>Xing Wang</b>, Xingyu Chen, Zhijie Wang, Jiahao Xu, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Rui Wang. <font color="#0080FF">RaSA: Rank-Sharing Low-Rank Adaptation</font>. <i>ICLR 2025</i>. (<a href="http://xingwang4nlp.com/">pdf</a>)[<a href="http://xingwang4nlp.com/">code</a>]
</li>
</ul>

<ul>
<li type="circle">Jen-tse Huang, Eric John Li, Man Ho LAM, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang Jiao,  <b>Xing Wang</b>, Zhaopeng Tu, Michael Lyu. <font color="#0080FF">Competing Large Language Models in Multi-Agent Gaming Environments</font>. <i>ICLR 2025</i>. (<a href="http://xingwang4nlp.com/">pdf</a>)[<a href="http://xingwang4nlp.com/">code</a>]
</li>
</ul>


<ul>
<li type="circle">Jiahui Li, Yongchang Hao, Haoyu Xu, <b>Xing Wang</b>*, Yu Hong. <font color="#0080FF">Exploiting the Index Gradients for Optimization-Based Jailbreaking on Large Language Models</font>. <i>COLING 2025</i>. (<a href="https://aclanthology.org/2025.coling-main.305/">pdf</a>)[<a href="https://github.com/jiah-li/magic">code</a>]
</li>
</ul>

<ul>
<li type="circle">Xiaolin Xing, Zhiwei He, Haoyu Xu, <b>Xing Wang</b>*, Rui Wang, Yu Hong. <font color="#0080FF">Evaluating Knowledge-based Cross-lingual Inconsistency in Large Language Models</font>. <i>arxiv 2024</i>. (<a href="https://arxiv.org/abs/2407.01358">pdf</a>)[<a href="https://github.com/Xingxl2studious/Cross-lingual-Consistency">code</a>]
</li>
</ul>

<ul>
<li type="circle">Xinming Hou, Mingming Yang, Wenxiang Jiao, <b>Xing Wang</b>*, Zhaopeng Tu, Wayne Xin Zhao. <font color="#0080FF">CoAct: A Global-Local Hierarchy for Autonomous Agent Collaboration</font>. <i>arxiv 2024</i>. (<a href="https://arxiv.org/abs/2406.13381">pdf</a>)[<a href="https://github.com/xmhou2002/CoAct">code</a>]
</li>
</ul>

<ul>
<li type="circle">Zhiwei He, Binglin Zhou, Hongkun Hao, Aiwei Liu, <b>Xing Wang</b>*, Zhaopeng Tu, Zhuosheng Zhang, Rui Wang. <font color="#0080FF">Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models</font>. <i>ACL 2024</i>. (<a href="https://arxiv.org/abs/2402.14007">pdf</a>)[<a href="https://github.com/zwhe99/X-SIR">code</a>]
</li>
</ul>

<ul>
<li type="circle">Tian Liang, Zhiwei He, Jen-tse Huang, Wenxuan Wang, Wenxiang Jiao, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, <b>Xing Wang</b>*. <font color="#0080FF">Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models</font>. <i>arxiv 2023</i>. (<a href="https://arxiv.org/pdf/2310.20499.pdf">pdf</a>)[<a href="https://github.com/Skytliang/SpyGame/">code</a>]
</li>
</ul>

<ul>
<li type="circle">Tian Liang, Zhiwei He, Wenxiang Jiao, <b>Xing Wang</b>*, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi. <font color="#0080FF">Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate</font>. <i>EMNLP 2024</i>. (<a href="https://arxiv.org/pdf/2305.19118.pdf">pdf</a>)[<a href="https://github.com/Skytliang/Multi-Agents-Debate">code</a>]
</li></ul>

<ul>
<li type="circle">Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, <b>Xing Wang</b>. <font color="#0080FF">Exploring Human-Like Translation Strategy with Large Language Models</font>. <i> Transaction of ACL 2024</i>. (<a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00642/119992/Exploring-Human-Like-Translation-Strategy-with">pdf</a>)[<a href="https://github.com/zwhe99/MAPS-mt">code</a>]{<a href="https://slator.com/how-large-language-models-mimic-human-translation-process/">media coverage</a>}
</li></ul>


<ul>
<li type="circle">Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang,  Zhiwei He, Tian Liang, <b>Xing Wang</b>, Shuming Shi, Zhaopeng Tu. <font color="#0080FF">ParroT: Translating during Chat using Large Language Models tuned with Human Translation and Feedback</font>. <i>Findings of EMNLP 2023</i>. (<a href="https://arxiv.org/pdf/2304.02426.pdf">pdf</a>)[<a href="https://github.com/wxjiao/ParroT">code</a>]
</li></ul>

<ul>
<li type="circle">Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, <b>Xing Wang</b>, Zhaopeng Tu. <font color="#0080FF">Is ChatGPT A Good Translator? A Preliminary Study</font>. <i>arxiv 2023</i>. (<a href="https://arxiv.org/pdf/2301.08745v2.pdf">pdf</a>)[<a href="https://github.com/wxjiao/Is-ChatGPT-A-Good-Translator">code</a>]{<a href="https://slator.com/tencent-pits-chatgpt-translation-quality-against-deepl-google-translate/">media coverage</a>}
</li></ul>


<h3>Neural Machine Translation</h3>
<ul>
<li type="circle">Tian Liang, <b>Xing Wang</b>*, Mingming Yang, Yujiu Yang, Shuming Shi, Zhaopeng Tu. <font color="#0080FF">Addressing Entity Translation Problem via Translation Difficulty and Context Diversity</font>. <i>Findings of ACL 2024</i>.[<a href="https://github.com/Skytliang/EntityTranslation">code</a>] 
</li></ul>

<ul>
<li type="circle">Zhiwei He, <b>Xing Wang</b>*, Wenxiang Jiao, Zhuosheng Zhang, Rui Wang, Shuming Shi, Zhaopeng Tu. <font color="#0080FF">Improving Machine Translation with Human Feedback: An Exploration of Quality Estimation as a Reward Model</font>. <i>NAACL 2024</i>. (<a href="https://arxiv.org/pdf/2401.12873.pdf">pdf</a>)[<a href="https://github.com/zwhe99/FeedbackMT">code</a>]
</li>
</ul>

<ul>
<li type="circle">Zhiwei He, <b>Xing Wang</b>, Rui Wang, Shuming Shi, Zhaopeng Tu. <font color="#0080FF">Bridging the Data Gap between Training and Inference for Unsupervised Neural Machine Translation</font>. <i>ACL 2022</i>. (<a href="https://aclanthology.org/2022.acl-long.456.pdf">pdf</a>)[<a href="https://github.com/zwhe99/SelfTraining4UNMT">code</a>]
</li></ul>

<ul>
<li type="circle">Zhiwei He, <b>Xing Wang</b>*, Zhaopeng Tu, Shuming Shi and Rui Wang. <font color="#0080FF">Tencent AI Lab - Shanghai Jiao Tong University Low-Resource Translation System for the WMT22 Translation Task
</font>. <i>WMT 2022</i>. (<a href="https://www.statmt.org/wmt22/pdf/2022.wmt-1.18.pdf">pdf</a>)[<a href="https://github.com/zwhe99/WMT22-En-Liv">code</a>]
</li></ul>

<ul>
<li type="circle">Wenxuan Wang, Wenxiang Jiao, Yongchang Hao, <b>Xing Wang</b>, Shuming Shi, Zhaopeng Tu, Michael Lyu. <font color="#0080FF">Understanding and Improving Sequence-to-Sequence Pretraining for Neural Machine Translation</font>. <i>ACL 2022</i>. (<a href="https://aclanthology.org/2022.acl-long.185.pdf">pdf</a>)
</li></ul>

<ul>
<li type="circle">Wenxiang Jiao, <b>Xing Wang</b>, Zhaopeng Tu, Shuming Shi, Michael Lyu and Irwin King. <font color="#0080FF">Self-training Sampling with Monolingual Data Uncertainty for Neural Machine Translation</font>. <i>ACL 2021</i>. (<a href="https://aclanthology.org/2021.acl-long.221.pdf">pdf</a>)[<a href="https://github.com/wxjiao/UncSamp">code</a>]
</li></ul>

<ul>
<li type="circle">Yongchang Hao, Shilin He, Wenxiang Jiao, Zhaopeng Tu, Michael Lyu, <b>Xing Wang</b>. <font color="#0080FF">Multi-Task Learning with Shared Encoder for Non-Autoregressive Machine Translation</font>. <i>NAACL 2021</i>. (<a href="https://aclanthology.org/2021.naacl-main.313.pdf">pdf</a>)[<a href="https://github.com/yongchanghao/multi-task-nat">code</a>]
</li></ul>
<ul>
<li type="circle">Wenxiang Jiao, <b>Xing Wang</b>, Shilin He, Irwin King, Michael Lyu and Zhaopeng Tu. <font color="#0080FF">Data Rejuvenation: Exploiting Inactive Training Examples for Neural Machine Translation</font>. <i>EMNLP 2020</i>. (<a href="https://www.aclweb.org/anthology/2020.emnlp-main.176.pdf">pdf</a>)[<a href="https://github.com/wxjiao/Data-Rejuvenation">code</a>]
</li></ul>

<ul>
<li type="circle"><b>Xing Wang</b>, Zhaopeng Tu, Deyi Xiong, Min Zhang. <font color="#0080FF">Translating Phrases in Neural Machine Translation</font>. <i>EMNLP 2017</i>.  
	(<a href="http://aclweb.org/anthology/D/D17/D17-1149.pdf">pdf</a>)
</li></ul>

<ul>
<li type="circle"><b>Xing Wang</b>, Zhengdong Lu, Zhaopeng Tu, Hang Li, Deyi Xiong, Min Zhang. <font color="#0080FF">Neural Machine Translation Advised by Statistical Machine Translation</font>. <i>AAAI 2017</i>. (<a href="http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14451/14257">pdf</a>) 
</li></ul>

<h3>Multimodal (Speech/Sign) Language Translation</h3>
<ul>
<li type="circle">Jinhui Ye, <b>Xing Wang</b>, Wenxiang Jiao, Junwei Liang, Hui Xiong. <font color="#0080FF">Improving Gloss-free Sign Language Translation by Reducing Representation Density</font>. <i>NeurIPS 2024</i>. (<a href="https://arxiv.org/abs/2405.14312">arxiv</a>)[<a href="https://github.com/JinhuiYE/SignCL">code</a>] 
</li></ul>


<ul>
<li type="circle">Zhengsheng Guo, Zhiwei He, Wenxiang Jiao,  <b>Xing Wang</b>, Rui Wang, Kehai Chen, Zhaopeng Tu, Yong Xu, Min Zhang. <font color="#0080FF">Unsupervised Sign Language Translation and Generation</font>. <i>Findings of ACL 2024</i>. (<a href="https://arxiv.org/abs/2402.07726">arxiv</a>)[<a href="https://github.com/ZhengshengGuo/USLNet">code</a>] 
</li></ul>

<ul>
<li type="circle">Yichao Du, Zhengsheng Guo, Jinchuan Tian, Zhirui Zhang, <b>Xing Wang</b>, Jianwei Yu, Zhaopeng Tu, Tong Xu, Enhong Chen. <font color="#0080FF">The MineTrans Systems for IWSLT 2023 Offline Speech Translation and Speech-to-Speech Translation Tasks</font>. <i>IWSLT 2023</i>. (<a href="https://aclanthology.org/2023.iwslt-1.3.pdf">pdf</a>)[<a href="https://github.com/duyichao/MINETrans-IWSLT23">code</a>] 
</li></ul>

<ul>
<li type="circle">Jinhui Ye, Wenxiang Jiao, <b>Xing Wang</b>*, Zhaopeng Tu, Hui Xiong. <font color="#0080FF">Cross-modality Data Augmentation for End-to-End Sign Language Translation</font>. <i>Findings of EMNLP 2023</i>. (<a href="https://arxiv.org/pdf/2305.11096.pdf">arxiv</a>)[<a href="https://github.com/Atrewin/SignXmDA">code</a>] 
</li></ul>

<ul>
<li type="circle">Jinhui Ye, Wenxiang Jiao, <b>Xing Wang</b>*, Zhaopeng Tu. <font color="#0080FF">Scaling Back-Translation with Domain Text Generation for Sign Language Gloss Translation</font>. <i>EACL 2023</i>. (<a href="https://arxiv.org/pdf/2210.07054.pdf">arxiv</a>)[<a href="https://github.com/Atrewin/PGen">code</a>] 
</li></ul>


<h3>Biomedical NLP</h3>

<ul>
<li type="circle">Jinghang Gu, Emmanuele Chersoni, <b>Xing Wang</b>, Chu-Ren Huang, Longhua Qian, Guodong Zhou. <font color="#0080FF">LitCovid ensemble learning for COVID-19 multi-label classification</font>. <i>Database 2022</i>. (<a href="https://academic.oup.com/database/article/doi/10.1093/database/baac103/6846687">html</a>)[<a href="https://github.com/JHnlp/LCEL">code</a>] [impact factor: 4.462]
</li></ul>


<ul>
<li type="circle">Jinghang Gu, Rong Xiang, <b>Xing Wang</b>, Jing Li, Wenjie Li, Longhua Qian, Guodong Zhou and Chu-Ren Huang. <font color="#0080FF">Multi-probe attention neural network for COVID-19 semantic indexing</font>. <i>BMC Bioinformatics 2022</i>. (<a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-022-04803-x">html</a>)[<a href="https://github.com/JHnlp/MPANN">code</a>] [impact factor: 3.169]
</li></ul>


<ul>
<li type="circle"><b>Xing Wang</b>, Zhaopeng Tu and Shuming Shi. <font color="#0080FF">Tencent AI Lab Machine Translation Systems for the WMT21 Biomedical Translation Task</font>. <i>WMT 2021</i>. (<a href="https://aclanthology.org/2021.wmt-1.89.pdf">pdf</a>)
</li></ul>


<ul>
<li type="circle"><b>Xing Wang</b>, Zhaopeng Tu, Longyue Wang and Shuming Shi. <font color="#0080FF">Tencent AI Lab Machine Translation Systems for the WMT20 Biomedical Translation Task</font>. <i>WMT 2020</i>. (<a href="http://www.statmt.org/wmt20/pdf/2020.wmt-1.97.pdf">pdf</a>)[<a href="https://github.com/hsing-wang/WMT2020_BioMedical">code</a>]
</li></ul>


</td>
</tr>

<tr>
<td align="left" valign="top"><hr color="#000000" size="1">
<h2>Talk</h2>

<ul>
<li type="circle">Invited Talk at School of Computer Science &amp; Technology of Soochow University, 2022/09 
</li></ul>

<ul>
<li type="circle">Invited Tutorial at <a href="http://sc.cipsc.org.cn/mt/conference/2022/">The 18th China Conference on Machine Translation</a>, 2022/08 
</li></ul>

<ul>
<li type="circle">Invited Talk at 2022 Tencent Academic and Industrial Conference, 2022/06
</li></ul>



</td>
</tr>

<tr>
<td align="left" valign="top"><hr color="#000000" size="1">
<h2>Shared Task</h2>
<ul>
<li type="circle">IWSLT 2023
<ul>
<li type="square">Speech-to-Speech Translation: ranked <b>No.1</b> for the end-to-end English to Chinese task. (<a href="https://github.com/duyichao/MINETrans-IWSLT23">project page</a>)</li> 
</ul>
</li> 
</ul>


<ul>
<li type="circle">WMT 2022
<ul>
<li type="square">General Translation: ranked <b>No.1</b> for the extremely low-resource task Livonian-English. (<a href="https://github.com/zwhe99/WMT22-En-Liv">project page</a>)</li> 
</ul>
</li> 
</ul>


<ul>
<li type="circle">WMT 2021  
<ul>
<li type="square">Biomedical Translation: ranked <b>No.1</b> for the German/French/Spanish-English tasks.</li> 
<li type="square">News Translation: ranked <b>No.1</b> for the Chinese-English task, ranked No.2 for the German-English task.</li> 
</ul>
</li> 
</ul>


<ul>
<li type="circle">WMT 2020  
<ul>
<li type="square">Biomedical Translation: ranked <b>No.1</b> for the German-English task, ranked No.3 for the English-German task. (<a href="https://github.com/hsing-wang/WMT2020_BioMedical">project page</a>, <a href="http://www.statmt.org/wmt20/pdf/2020.wmt-1.97.pdf">system report</a>)</li> 
<li type="square">News Translation: ranked No.2 for the Chinese-English task, ranked No.2 for the English-Chinese task. (<a href="http://www.statmt.org/wmt20/pdf/2020.wmt-1.34.pdf">system report</a>)</li> 
<li type="square">Chats Translation: ranked No.2 for the German-English task. (<a href="http://www.statmt.org/wmt20/pdf/2020.wmt-1.60.pdf">system report</a>)</li> 
</ul>
</li> 
</ul>


</td></tr>

<tr>
<td align="left" valign="top"><hr color="#000000" size="1">
<h2>Experience</h2>
<ul>
<li type="circle">2019/09 - present.  &nbsp;&nbsp; Senior Researcher at <a href="https://ai.tencent.com/ailab/index.html">Tencent AI Lab</a>, Shenzhen.</li>
<li type="circle">2018/07 - 2019/09.  &nbsp;&nbsp; Researcher at <a href="https://ai.tencent.com/ailab/index.html">Tencent AI Lab</a>, Shenzhen.</li>
<li type="circle">2017/08 - 2018/02.  &nbsp;&nbsp; Research intern at <a href="https://ai.tencent.com/ailab/index.html">Tencent AI Lab</a>, Shenzhen.</li>
<li type="circle">2016/04 - 2016/10.  &nbsp;&nbsp; Research intern at <a href="http://www.noahlab.com.hk/">Noahs Ark Lab, Huawei Technologies</a>, Hong Kong.</li>
<li type="circle">2012/09 - 2013/09.  &nbsp;&nbsp; Visiting student at <a href="http://nlp.ict.ac.cn/">NLP Group</a>, <a href="http://www.ict.ac.cn/">ICT</a>, <a href="http://www.cas.cn/">CAS</a>, Beijing.  </li>

</ul>
</td>
</tr>

<tr>
<td align="left" valign="top"><hr color="#000000" size="1">
<h2>Project</h2>
<ul>
<li type="circle"><a href="https://cloud.tencent.com/developer/article/1879377?from=article.detail.1746685">Tencent AI Lab RhinoBird Focused Research Program</a>,  Technology Innovation Award (top 10%)  (2020/08 - 2021/08, CO-PI)</li>
<li type="circle"><a href="https://www.ccf.org.cn/Collaboration/Enterprise_Fund/News/tx/2022-02-28/756316.shtml">CCF-Tencent Rhino-Bird Young Faculty Open Research Fund</a> (2021/10 - 2022/10, CO-PI)</li>
<li type="circle"><a href="http://xingwang4nlp.com/">Tencent AI Lab RhinoBird Focused Research Program</a>,   (2023/08 - 2024/08, CO-PI)</li>
<li type="circle"><a href="http://xingwang4nlp.com/">Tencent AI Lab RhinoBird Focused Research Program</a>,   (2024/08 - 2025/08, CO-PI)</li>


</ul>
</td>
</tr>


<tr>
<td align="left" valign="top"><hr color="#000000" size="1">
<h2>Research Intern (co-supervised)</h2>
<ul>
<li type="circle"><a href="https://demon-jiehao.github.io/">Jie Hao</a> (PhD student from Florida State University, 2018/07 - 2019/05)</li>
<li type="circle"><a href="https://shilinhe.github.io/">Shilin He</a> (PhD student from Chinese University of Hong Kong, 2019/04 - 2019/06)</li>
<li type="circle"><a href="https://wxjiao.github.io/">Wenxiang Jiao</a> (PhD student from Chinese University of Hong Kong, 2019/09 - 2021/08)</li>
<li type="circle"><a href="https://yongchanghao.github.io/">Yongchang Hao</a> (Undergraduate student from Soochow University, 2020/02 - 2021/07)</li>
<li type="circle"><a href="http://xingwang4nlp.com/">Jiaqing Zhang</a> (Master student from University of Florida, 2020/08 - 2021/08)</li>
<li type="circle"><a href="https://zwhe99.github.io/">Zhiwei He</a> (Undergraduate student from South China University of Technology, PhD student from Shanghai Jiao Tong University, 2021/03 - present)</li>
<li type="circle"><a href="https://jhuiye.com/">Jinhui Ye</a> (Undergraduate student from South China University of Technology, Master student from The Hong Kong University of Science and Technology (Guangzhou), 2021/08 - 2022/09)</li>
<li type="circle"><a href="https://skytliang.github.io/">Tian Liang</a> (Master student from Tsinghua Shenzhen International Graduate School, 2022/03 - 2024/05) Tencent Rhino-Bird Elite Training Program 2022-2023</li>
<li type="circle"><a href="http://xingwang4nlp.com/">Zhengsheng Guo</a> (Master student Harbin Institute of Technology, Shenzhen, 2022/03-2023/08)</li>
<li type="circle"><a href="http://xingwang4nlp.com/">Junjie Yu</a> (PhD student  from Soochow University, 2021/05 - 2024/06)</li>
<li type="circle"><a href="http://xingwang4nlp.com/">Xiaolin Xing</a> (Master student from Soochow University, 2023/06 - 2024/06)</li>
<li type="circle"><a href="http://xingwang4nlp.com/">Haoyu Xu</a> (Master student from Soochow University, 2023/06 - present)</li>
<li type="circle"><a href="http://xingwang4nlp.com/">Jiahui Li</a> (Master student from Soochow University, 2023/06 - present)</li>
<li type="circle"><a href="http://xingwang4nlp.com/">Zicheng Lin</a> (Master student from Tsinghua Shenzhen International Graduate School, 2024/06 - present)</li>

</ul>
</td>
</tr>


<tr>
<td align="left" valign="top"><hr color="#000000" size="1">
<h2>Professional Service</h2>
<ul>
<li type="circle"><b>Area Chair</b>: ACL(2024), EMNLP(2024), NAACL(2024), CCL(2024), ICLR(2025).</li>
<li type="circle"><b>Conference PC Member or Reviewer</b>: ACL, ACL ARR, NeurIPS, ICML, EMNLP, NAACL, AAAI, IJCAI, COLING, CCL, NLPCC, CWMT.</li>
<li type="circle"><b>Journal Reviewer</b>:  Artificial Intelligence, Transactions on Audio, Speech, and Language Processing, Transactions of the Association for Computational Linguistics, Neurocomputing, Transactions on Asian and Low-Resource Language Information Processing, Pattern Recognition Letters, Journal of Computer Science and Technology.</li>

</ul>
</td>
</tr>


<tr>
<td align="left" valign="top"><hr color="#000000" size="1">
*Last updated on 2025/01/24.
</td>
</tr>
</tbody></table>
</div>




</td></tr></tbody></table></div></body></html>